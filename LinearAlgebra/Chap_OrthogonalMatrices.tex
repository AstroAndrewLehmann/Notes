\chapter{Orthogonal matrices} \label{ch:orthogonal}


\section{Orthogonal diagonalization}

\definition{Orthogonal matrix}{
A square matrix $A$ is orthogonal if and only if its inverse is its transpose. That is, if and only if $A A^T = A^T A = I$.
}

\theorem{Transition matrix of orthonormal bases}{
Consider an inner product space $V$ and two orthonormal bases $\mathcal{A}$ and $\mathcal{B}$. Then the transition matrix $P_{\mathcal{A}\to\mathcal{B}}$ is an orthogonal matrix.
}

\theorem{Matrix orthogonality}{
An $n \times n$ matrix $A$ is an orthogonal matrix if and only if its columns (considered as $n$-tuples) form an orthonormal basis of $\mathbb{R}^n$. This result also holds for the rows of $A$.
}

\properties{of orthogonal matrices}{
If $A$ is an orthogonal matrix of size $n$, then
\begin{itemize}
	\item its columns are pair-wise orthogonal,
	\item its columns are unit length,
	\item its columns (considered as $n$-tuples) form an orthonormal basis of $\mathbb{R}^n$,
	\item its rows (considered as $n$-tuples) form an orthonormal basis of $\mathbb{R}^n$,
	\item it has determinant $\pm 1$.
\end{itemize}
}


\definition{Orthogonally diagonalizable matrix}{
A square matrix $A$ is orthogonally diagonalizable if there exists a diagonal matrix $D$ and orthogonal matrix $Q$ such that
\begin{align*}
A = Q D Q^T.
\end{align*}
}

\section{Symmetric real matrices}

Let's remind ourselves of the definition of this type of matrix.

\definition{Symmetric matrix}{A matrix $A$ is symmetric if and only if it is equal to its own transpose:
\begin{align*}
A = A^T
\end{align*}
}

And let's introduce another related matrix:

\definition{Skew-symmetric matrix}{A matrix $A$ is skew-symmetric if and only if it is equal to the negative of its transpose:
\begin{align*}
A = -A^T
\end{align*}
}

\theorem{Spectral completeness of symmetric real matrices}{
Every $n \times n$ symmetric real matrix has $n$ linearly independent eigenvectors.
}

\theorem{Diagonalizability of symmetric real matrices}{
Every symmetric real matrix is orthogonally diagonalizable.
}

\theorem{Orthogonality of eigenvectors of symmetric real matrices}{
Let $\lambda_1$ and $\lambda_2$ be distinct eigenvalues of a symmetric real matrix. Every eigenvector belonging to the eigenspace corresponding to $\lambda_1$ is orthogonal to every eigenvector belonging to the eigenspace corresponding to $\lambda_2$.
}

\definition{Quadratic form}{
Let $A$ be an $n \times n$ matrix and $\mathbf{v} \in \mathbf{R}^n$ \textit{considered as a column}. Then a quadratic form is a multiplication of the form $\mathbf{v}^T A  \mathbf{v}$ resulting in a real number.
}

\definition{Definite matrix}{
Let $A$ be an $n \times n$ symmetric real matrix. By considering the sign of quadratic forms with $A$ we can define several cases. $A$ is
\begin{itemize}
 \item \textit{positive definite} if an only if $\mathbf{v}^T A \mathbf{v} > 0$ for every $\mathbf{v}\in\mathbb{R}^n$,
 \item \textit{positive semi-definite} if an only if $\mathbf{v}^T A \mathbf{v} \geq 0$ for every $\mathbf{v}\in\mathbb{R}^n$,
 \item \textit{negative definite} if an only if $\mathbf{v}^T A \mathbf{v} < 0$ for every $\mathbf{v}\in\mathbb{R}^n$,
 \item \textit{negative semi-definite} if an only if $\mathbf{v}^T A \mathbf{v} \leq 0$ for every $\mathbf{v}\in\mathbb{R}^n$.
\end{itemize}
If the matrix does not satisfy any of these (e.g. if we can find a positive and a negative quadratic form) then the matrix is called \textit{indefinite}.
}


\theorem{Eigenvalues of a definite matrix}{
Let $A$ be an $n \times n$ symmetric real matrix. Then all eigenvalues of $A$ are real numbers. Furthermore, $A$ is
\begin{itemize}
 \item \textit{positive definite} if an only if every eigenvalue is strictly positive,
 \item \textit{positive semi-definite} if an only if every eigenvalue is non-negative,
 \item \textit{negative definite} if an only if every eigenvalue is strictly negative,
 \item \textit{negative semi-definite} if an only if every eigenvalue is strictly non-positive.
\end{itemize}
}

\theorem{Sylvester's criterion}{
An $n \times n$ symmetric real matrix is \textit{positive definite} if and only if all of the $n$ submatrices formed by the possible squares starting in the upper left corner, called leading principal minors, have positive determinant. For example, for a $4 \times 4$ matrix
\begin{align*}
A =
\begin{pmatrix}
a_{11} & a_{12} & a_{13} & a_{14} \\
a_{21} & a_{22} & a_{23} & a_{24} \\
a_{31} & a_{32} & a_{33} & a_{34} \\
a_{41} & a_{42} & a_{43} & a_{44}
\end{pmatrix}
\end{align*}
we must check the sign of 4 determinants:
\begin{align*}
\det\begin{pmatrix} a_{11} \end{pmatrix}, 
\quad 
\det\begin{pmatrix} a_{11} & a_{12} \\ a_{21} & a_{22} \end{pmatrix}, 
\quad 
\det\begin{pmatrix} 
a_{11} & a_{12} & a_{13} \\
a_{21} & a_{22} & a_{23} \\
a_{31} & a_{32} & a_{33} 
\end{pmatrix}, 
\quad 
\det(A).
\end{align*}
}

