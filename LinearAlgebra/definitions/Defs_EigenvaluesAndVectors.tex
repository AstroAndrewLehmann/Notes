\chapter{Eigenvalues and Eigenvectors} \label{ch:eigens}


\definition{Eigenvalues and eigenvectors of a linear map}{
Consider an endomorphism $f:V \to V$ and a vector $\mathbf{v}\in V$. We call a number $\lambda \neq 0$ an eigenvalue of $f$ if there exists a non-zero vector $\mathbf{v}$ satisfying the relation
\begin{align*}
f(\mathbf{v}) = \lambda \mathbf{v}.
\end{align*}
We say that $\mathbf{v}$ is an eigenvector of $f$ \textit{corresponding} or \textit{associated} to the eigenvalue $\lambda$.
}


\definition{Eigenvectors and eigenvalues of a matrix}{
For a square matrix $A$, an eigenvector of $A$ is a non-zero vector, $\mathbf{v}$, that satisfies the matrix equation
\begin{align*}
A\mathbf{v} = \lambda \mathbf{v}
\end{align*}
where $\lambda$ is called an eigenvalue of $A$. We say that $\mathbf{v}$ is an eigenvector of $A$ \textit{corresponding} or \textit{associated} to the eigenvalue $\lambda$.
}


\definition{Characteristic polynomial of a matrix}{
For a square matrix $A$, the \textit{characteristic polynomial} is the given by
\begin{align*}
P(\lambda) = \det ( A - \lambda I)
\end{align*}
where $I$ is the identity matrix with the same size as $A$ and $\lambda$ is the variable of the polynomial. The degree of this polynomial is always the same as the size of the matrix $A$.
}


\definition{Eigenspectrum}{
The eigenvalues of a square matrix $A$ are roots of the characteristic polynomial of $A$. That is, eigenvalues are solutions of
\begin{align*}
 \det ( A - \lambda I) = 0.
\end{align*} 
There can be multiple distinct eigenvalues of $A$, and are conventionally denoted $\lambda_1$, $\lambda_2$, \dots etc. The set of these eigenvalues, $\{\lambda_1, \, \lambda_2, \dots \}$, is called the \textit{eigenspectrum} of $A$.
}


\definition{Eigenspace}{
For any eigenvalue $\lambda_k$ of an $n \times n$ matrix $A$, the eigenspace corresponding to $\lambda_k$, denoted $E_{\lambda_k}$, is the set of all eigenvectors corresponding to $\lambda_k$. This can be written as the set of linear combinations of linearly independent eigenvectors corresponding to $\lambda_k$:
\begin{gather*}
E_{\lambda_k} = \{ \alpha_1 \mathbf{v}_1 + \cdots + \alpha_m \mathbf{v}_m \, | \, \forall j \, A \mathbf{v}_j = \lambda_k \mathbf{v}_j, \, \alpha_j \in \mathbb{R} \} = \text{SPAN}(\mathbf{v}_1, \cdots, \mathbf{v}_m) \\
\text{for maximum number of eigenvectors such that} \\
\alpha_1 \mathbf{v}_1 + \cdots + \alpha_m \mathbf{v}_m = \mathbf{0} \implies  \alpha_1 =  \alpha_2 = \cdots = \alpha_m = 0.
\end{gather*}
As the set $\{\mathbf{v}_1, \cdots, \mathbf{v}_m\}$ generates $E_{\lambda_k}$ and the vectors are linearly independent, the set forms a basis and therefore gives the dimension $E_{\lambda_k}$.

The eigenspace can also be written like a \textit{kernel}
\begin{align*}
E_{\lambda_k} = \{ \mathbf{v} \in \mathbb{R}^n \, | \, \left( A - \lambda_k I \right) \mathbf{v}= \mathbf{0} \}.
\end{align*}
}


\definition{Algebraic and geometric multiplicity of an eigenvalue}{
For an $n \times n$ matrix with characteristic polynomial
\begin{align*}
P(\lambda) = C(\lambda - \lambda_1)^{m_1} \times \cdot \times (\lambda - \lambda_k)^{m_k}\times  \cdot \times (\lambda - \lambda_p)^{m_p}
\end{align*}
for some constant $C$. There can be up to $n$ distinct eigenvalues ($p \leq n$). The exponent $m_k$ is called the \textit{algebraic} multiplicity of the eigenvalue $\lambda_k$. The \textit{dimension} of the eigenspace corresponding to $\lambda_k$ is its \textit{geometric} multiplicity.
}



\definition{Eigenbasis}{
Consider a square matrix $A$ of size $n$. If the dimensions of its eigenspaces add up to $n$, then there exist $n$ linearly independent eigenvectors of $A$. These eigenvectors form a basis of $\mathbb{R}^n$ called an \textit{eigenbasis}.
}


\definition{Similar matrices}{
Two matrices $A$ and $B$ are similar if there exists an invertible matrix $P$ such that
\begin{align*}
B = P A P^{-1}.
\end{align*}
}


\definition{Diagonalizable linear map}{
Let $f:V\to V$ be an endomorphism. $f$ is called \textit{diagonalizable} if there exists a basis, $\mathcal{B}$, of $V$ such that the matrix representation of $f$ in $\mathcal{B}$ is diagonal:
\begin{align*}
(\mathcal{M}(f,\mathcal{B}))_{ij} = 0 \quad \text{whenever } \, i \neq j.
\end{align*}
}

\definition{Diagonalizable matrix}{
A square matrix $A$ is \textit{diagonalizable} if and only if there exists an invertible matrix $P$ and diagonal matrix $D$ such that
\begin{align*}
A = P D P^{-1}.
\end{align*}
Alternative: A square matrix $A$ is \textit{diagonalizable} if and only if it is \textit{similar} to a diagonal matrix $D$. 
}

\definition{Eigenvalue diagonalization}{
For a diagonalizable matrix $A$ of size $n$, we can sometimes find a diagonal matrix consisting of the eigenvalues of $A$, $\lambda_1, \, \dots, \, \lambda_n$. In this case we can write
\begin{align*}
A = P D P^{-1}
\end{align*}
where $P$ consists of eigenvectors of $A$ as columns. The matrix $P$ is the transition matrix from the eigenbasis, $\mathcal{E}$, to the canonical basis of $\mathbb{R}^n$: $P_{\mathcal{E}\to \mathcal{C}_n}$.
}
