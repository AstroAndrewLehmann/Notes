\chapter{Linear Maps} \label{ch:linearmaps}

\definition{Linear map}{
A mapping, $f$, from a vector space $V$ to a vector space $W$, denoted $f:V \to W$, is called a \textit{linear map} if it satisfies the following property:
\begin{align*}
& \forall \mathbf{u}, \, \mathbf{v} \in V, \, \forall \alpha,\beta \in \mathbb{R} \\
& f(\alpha\mathbf{u} + \beta\mathbf{v}) = \alpha f(\mathbf{u}) + \beta f(\mathbf{v}).
\end{align*}
We say that a linear map \textit{preserves linear combinations}.
}


\definition{Image}{
The \textit{image of a linear map} $f: V \to W$, denoted $\text{im}(f)$, is the set of all possible ``output'' vectors of the map:
\begin{align*}
\text{im}(f) = \{ \mathbf{w} \in W \,\, | \,\, \exists \mathbf{v}\in V\, f(\mathbf{v}) = \mathbf{w} \} \subseteq W.
\end{align*}
}


\definition{Rank}{
The \textit{rank of a linear map} is the dimension of its image: $\text{rank}(f)=\dim(\text{im}(f))$.
}


\definition{Kernel}{
The \textit{kernel of a linear map}  $f: V \to W$, denoted $\ker(f)$, is the set of vectors that $f$ maps to the zero vector, $\mathbf{0}_W$, of $W$. That is,
\begin{align*}
\ker(f) = \{\mathbf{v} \in V \,\, | \,\, f(\mathbf{v}) = \mathbf{0}_W \}.
\end{align*}
}


\definition{Nullity}{
The \textit{nullity of a linear map} is the dimension of its kernel: $\text{nullity}(f)=\dim(\ker(f))$.
}





\definition{Injectivity}{
Let $f:V\to W$ be a linear map. We say $f$ is injective if no two vectors of $V$ are mapped to the same vector of $W$. In symbols we have two equivalent expressions
\begin{gather*}
\forall \, \mathbf{x},\mathbf{y}\in V, \quad \left(f(\mathbf{x})=f(\mathbf{y}) \implies \mathbf{x}=\mathbf{y}\right) \\
%
\text{or} \\
%
\forall \, \mathbf{x},\mathbf{y}\in V, \quad \left( \mathbf{x} \neq \mathbf{y} \implies f(\mathbf{x}) \neq f(\mathbf{y})\right)  
\end{gather*}
}

\definition{Surjectivity}{
Let $f:V\to W$ be a linear map. We say that $f$ is surjective if every vector in the output space has a corresponding input vector. In symbols
\begin{align*}
\forall \, \mathbf{w} \in W \quad \exists \mathbf{v}\in V \, \text{such that} \, f(\mathbf{v})=\mathbf{w}.
\end{align*}
}

\definition{Categories of linear maps}{
Let $f:V\to W$ be a linear map.
\begin{itemize}
\item If $W=V$ we call $f$ an \textit{endomorphism}.
\item If $f$ is both injective and surjective then we say it is bijective and we call it an \textit{isomorphism}.
\item If $f$ is both an isomorphism and an endomorphism we call it an \textit{automorphism}.
\end{itemize}
}

\definition{Composition of linear maps}{
Composition of linear maps works exactly as you would expect if you remember the composition of regular functions. We must have a coherence between the output of one linear map and the input of another. So, two linear maps $f:A\to B$ and $g:U\to V$ can be composed as a well defined linear map $g\circ f$ (``$g$ of $f$'') if and only if the output space of $f$ is the input space of $g$: $U=B$. For any $\mathbf{u}\in A$ the composition is written
\begin{align*}
g\circ f: A \to V \quad \text{and} \quad (g\circ f)(\mathbf{u}) = g(f(\mathbf{u})).
\end{align*}
}



\definition{Matrix representation of a linear map}{
Let $f:V \to W$ be a linear map, $\mathcal{A}=\{\mathbf{a}_1,\dots,\mathbf{a}_n\}$ be a basis of $V$, $\mathcal{B}=\{\mathbf{b}_1,\dots,\mathbf{b}_m\}$ be a basis of $W$. Let $\mathbf{v}$ be any vector in $V$ and $\mathbf{w}=f(\mathbf{v})\in W$. Then the \textit{matrix representation} of $f$ in bases $\mathcal{A}$ and $\mathcal{B}$, defined by the unique $m\times n$ matrix, denoted $\mathcal{M}(f,\mathcal{A}\to\mathcal{B})$, which takes the coordinates of $\mathbf{v}$ to the coordinates of $\mathbf{w}$ in their respective bases:
\begin{align*}
\mathcal{M}(f,\mathcal{A}\to\mathcal{B})[\mathbf{v}]_\mathcal{A} = [\mathbf{w}]_\mathcal{B}.
\end{align*}
can be calculated by expressing the coordinates of the linear map acting on the basis vectors of the input space
\begin{align*}
\mathcal{M}(f,\mathcal{A}\to\mathcal{B})=
\begin{pmatrix}
| &  & | \\
[f(\mathbf{a}_1)]_\mathcal{B} & \dots & [f(\mathbf{a}_n)]_\mathcal{B}\\
| &  & | 
\end{pmatrix}
\end{align*}
where the vertical lines are reminders that the coordinates of the $f(\mathbf{a}_k)$ vectors are columns. We often shorten ``matrix representation of $f$'' to just ``matrix of $f$''. If the input and output vector spaces are the same, i.e. if $f$ is an endomorphism, we can use the same basis for both spaces and we may shorten the notation $\mathcal{M}(f,\mathcal{A}\to\mathcal{A}) = \mathcal{M}(f,\mathcal{A})$.
}


\definition{Transition matrix (change-of-basis matrix)}{
The \textit{transition matrix} changes the representation of the coordinates of a vector from one basis into another. Let $\mathcal{A}$ and $\mathcal{B}$ be two bases of the same vector space, $V$, and let $\mathbf{v} \in V$. The transition matrix from $\mathcal{A}$ to $\mathcal{B}$, denoted $P_{\mathcal{A}\to \mathcal{B}}$ is defined by the relation
\begin{align*}
P_{\mathcal{A}\to \mathcal{B}} [\mathbf{v}]_\mathcal{A} = [\mathbf{v}]_\mathcal{B}.
\end{align*}
If we let the bases $\mathcal{A}=\{\mathbf{a}_1,\dots,\mathbf{a}_n\}$ and $\mathcal{B}=\{\mathbf{b}_1,\dots,\mathbf{b}_m\}$ then the transition matrix can be calculated by
\begin{align*}
P_{\mathcal{A}\to\mathcal{B}} =
\begin{pmatrix}
| & | & & | \\
[\mathbf{a}_1]_\mathcal{B} & [\mathbf{a}_2]_\mathcal{B} & \dots & [\mathbf{a}_n]_\mathcal{B}\\
| & | & & | 
\end{pmatrix}
\end{align*}
where the vertical lines are reminders that the coordinates of the $A$ basis vectors are columns.
}


\definition{Changing the bases of a matrix representation}{
Let $f:U \to V$ be a linear map, $\mathcal{B}_U$ and $\mathcal{B}'_U$ be two bases of $U$, $\mathcal{B}_V$ and $\mathcal{B}'_V$ be two bases of $V$, and $F=\mathcal{M}(f,\mathcal{B}_U\to\mathcal{B}_V)$ be the matrix representation of $f$ from basis $\mathcal{B}_U$ to basis $\mathcal{B}_V$. 

\noindent Then $F'=\mathcal{M}(f,\mathcal{B}'_U\to\mathcal{B}'_V)$, the matrix representation of $f$ from basis $\mathcal{B}'_U$ to basis $\mathcal{B}'_V$, is given by
\begin{align*}
F' = P_{\mathcal{B}_V\to\mathcal{B}_V'} \, F \, P_{\mathcal{B}'_U\to\mathcal{B}_U}
\end{align*}

\noindent The following diagram may help visualise this relation
\begin{figure}[H]
\centering
\begin{tikzpicture}
	\coordinate (U)  at (-2.5,+2);
	\coordinate (Ud) at (-2.5,-2);
	\coordinate (V)  at (+2.5,+2);
	\coordinate (Vd) at (+2.5,-2);
	
	\coordinate (F)  at (+0,+2.7);
	\coordinate (Fd) at (+0,-2.7);
	\coordinate (P1) at (-4,+0);
	\coordinate (P2) at (+4,+0);
	
	\node[black,scale=2.5] at (U)  {$\mathcal{B}_U$};
	\node[black,scale=2.5] at (Ud) {$\mathcal{B}'_U$};
	\node[black,scale=2.5] at (V)  {$\mathcal{B}_V$};
	\node[black,scale=2.5] at (Vd) {$\mathcal{B}'_V$};
	
	\node[black,scale=2.5] at (F)  {$F$};
	\node[black,scale=2.5] at (Fd) {$F'$};
	\node[black,scale=2] at (P1) {$P_{\mathcal{B}'_U\to\mathcal{B}_U}$};
	\node[black,scale=2] at (P2) {$P_{\mathcal{B}_V\to\mathcal{B}_V'}$};
	
	\draw[->,ultra thick] ($(U)+(1,0)$)--($(V)-(1,0)$);
	\draw[->,ultra thick] ($(Ud)+(1,0)$)--($(Vd)-(1,0)$);
	\draw[->,ultra thick] ($(Ud)+(0,0.8)$)--($(U)-(0,0.8)$);
	\draw[->,ultra thick] ($(V)-(0,0.8)$)--($(Vd)+(0,0.8)$);
\end{tikzpicture}
\end{figure}
To read this schematic, consider that the arrow for $F'$ has the same input and output as following the other three arrows to go up, then right (through $F$) then down again. This ordered path is the matrix multiplication given above.
}
