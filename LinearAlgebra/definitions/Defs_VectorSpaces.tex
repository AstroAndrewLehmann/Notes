\chapter{Vector Spaces} \label{ch:vectorspaces}


\definition{Vector space}{
A \textit{vector space over a field} $\mathbb{F}$ is a set, call it $V$, with elements called vectors supplied with definitions of two operations, \textit{vector addition} (VA) and \textit{scalar multiplication} (SM), that satisfy the following \textit{vector space axioms}:
\begin{align*}
& \forall \, \mathbf{u},\mathbf{v},\mathbf{w} \in V \quad \text{and} \quad \forall \, k,l \in \mathbb{F} \\
\text{(VA1)} & \quad \mathbf{u} + \mathbf{v} \in V  & (\text{closure under vector addition})\\
%
\text{(VA2)} & \quad (\mathbf{u} + \mathbf{v}) + \mathbf{w}  =\mathbf{u} + (\mathbf{v} + \mathbf{w} ) & (\text{associativity of vector addition})\\
%
\text{(VA3)} & \quad \exists \, \mathbf{0} \in V, \, \text{such that} \, \mathbf{u} + \mathbf{0} = \mathbf{0} + \mathbf{u} = \mathbf{u} & (\text{additive identity})\\
%
\text{(VA4)} & \quad \exists \, -\mathbf{u} \in V \, \text{such that} \, \mathbf{u} + (-\mathbf{u}) = \mathbf{0} & (\text{additive inverse})\\
%
\text{(VA5)} & \quad \mathbf{u} + \mathbf{v} = \mathbf{v} + \mathbf{u} & (\text{commutativity of vector addition})\\
%
\text{(SM1)} & \quad k\mathbf{u} \in V & (\text{closure under scalar multiplication})\\
%
\text{(SM2)} & \quad k(\mathbf{u}+\mathbf{v})=k\mathbf{u}+k\mathbf{v} & (\text{distributivity over vector addition})\\
%
\text{(SM3)} & \quad (k+l)\mathbf{u}=k\mathbf{u}+l\mathbf{u} & (\text{distributivity over field addition})\\
%
\text{(SM4)} & \quad k(l\mathbf{u})=(kl)\mathbf{u} & (\text{compatibility of scalar and field multiplication})\\
%
\text{(SM5)} & \quad 1\mathbf{u}=\mathbf{u} & (\text{multiplicative identity})
\end{align*}
}

\definition{Linear Combination}{
Let \{$\mathbf{v}_1$, \dots, $\mathbf{v}_n$\} be a set of vectors in a vector space $V$. A linear combination of these vectors is a new vector, $\mathbf{w}\in V$, of the form
\begin{align*}
\mathbf{w} = \alpha_1 \mathbf{v}_1 + \cdots + \alpha_n \mathbf{v}_n
\end{align*}
where the $\alpha_k$ are real numbers.
}


\definition{Vector subspace}{
Suppose that $V$ is a vector space and $W$ is a subset of $V$. We call $W$ a \textit{vector subspace} if it satisfies the vector space axioms for the same definition of vector addition and scalar multiplication defined for $V$.
}


\definition{Span}{
Let $\mathcal{B} = \{\mathbf{v}_1, \dots, \mathbf{v}_n\}$ be a set of vectors from a vector space $V$. The span of these vectors is the set of all linear combinations of those vectors:
\begin{align*}
\text{SPAN}(\mathcal{B})  = \text{SPAN} (\mathbf{v}_1, \dots, \mathbf{v}_n) = \left\{ \alpha_1 \mathbf{v}_1 + \cdots + \alpha_n \mathbf{v}_n \, | \, \forall \alpha_1, \dots, \alpha_n \in \mathbb{R}^n \right\}.
\end{align*}
This set forms a vector subspace of $V$. It is obviously non-empty because it at least contains the vectors of $\mathcal{B}$. It is also automatically closed under vector addition and scalar multiplication because those are exactly the operations we used to create all the vectors in the span! Therefore $\text{SPAN}(\mathcal{B})$ is a vector subspace of $V$.
}


\definition{Cartesian form of Euclidean vector sub spaces}{
Euclidean vector sub spaces can always be written as a set with some defining equations:
\begin{align*}
\left\{ (x_1,\dots,x_n) \in \mathbb{R}^n \, | \, \text{equations relating the } x_k \right\}.
\end{align*}
For example, the general form of planar vector subspaces of $\mathbb{R}^3$ is
\begin{align*}
V_P = \left\{ (x,y,z)\in \mathbb{R}^3 \, | \, ax + by + cz = 0\right\}
\end{align*}
where $a$, $b$ and $c$ are some given constants. This set is read aloud as ``all the triples $(x,y,z)$ such that $ax + by + cz = 0$''.
}


\definition{Sum of subspaces (sum space)}{
Suppose we have a vector space $V$ with vector subspaces $F$ and $G$. We define the \textbf{sum of subspaces} (or sum space) as a new set denoted
\begin{align*}
F + G = \left\{ \textbf{f} + \textbf{g} \, | \, \textbf{f}\in F, \, \textbf{g}\in G\right\}
\end{align*}

\noindent \textit{Note}: The sum space is a \textit{subset} of the parent vector space: $F+G \subset V$.
}


\definition{Direct sum}{
Let $F$ and $G$ be two vector subspaces of a vector space $V$ and let $E=F+G$ be the sum space. We say $E$ is a \textbf{direct sum} of $F$ and $G$ if each element of $E$ has a \textbf{unique} decomposition as a sum of vectors in $F$ and vectors in $G$. That is, for every $\textbf{v}\in E$, there exists unique vectors $\textbf{f}\in F$ and $\textbf{g}\in G$ such that $\textbf{v} = \textbf{f} + \textbf{g}$. We denote this direct sum with a new symbol
\begin{align*}
E = F \oplus G
\end{align*}
}


\definition{Complementary vector subspaces}{
Let $F$ and $G$ be two vector subspaces of $V$. $F$ and $G$ are called \textbf{complementary} if $V$ is a direct sum of $F$ and $G$. That is, if and only if
\begin{itemize}
\item $V = F+G$, and
\item $F \cap G = \{\textbf{0}_V \}$
\end{itemize}
}

\definition{Linear dependence}{
A set of vectors $\{\mathbf{v}_1, \dots, \mathbf{v}_n\}$ from a vector space $V$ is said to be \textit{linearly dependent} if there exists a set of constants $\{ \alpha_1, \dots, \alpha_n \}$ \textit{not all zero} such that
\begin{align*}
\alpha_1 \mathbf{v}_1 + \cdots + \alpha_n \mathbf{v}_n = \mathbf{0}_V.
\end{align*}
\textit{Note}: the right hand side of the equation is the \textit{zero vector}, not the real number $0$.
}

\definition{Linear independence}{
A set of vectors $\{\mathbf{v}_1, \dots, \mathbf{v}_n\}$ from $V$ is said to be \textit{linearly independent} if they are not linearly dependent. That is, the equation
\begin{align*}
\alpha_1 \mathbf{v}_1 + \cdots + \alpha_n \mathbf{v}_n =  \mathbf{0}_V.
\end{align*}
implies that the constants $\alpha_1, \dots, \alpha_n$ \textit{are all zero}.
}


\definition{Basis}{
A \textit{basis of a vector space} $V$ is a minimal set of vectors which spans the vector space. Formally, the set of vectors $\mathcal{B}=\{\mathbf{v}_1, \dots, \mathbf{v}_n\}$ in a vector space $V$ is a basis of $V$ if it is a set of linearly independent vectors and $\text{SPAN}(\mathbf{v}_1, \dots, \mathbf{v}_n) = V$. \textit{Note}: bases are not unique, but they always contain the same number of vectors.
}

\definition{Dimension}{
The \textit{dimension of a vector space} is the number of elements in a basis for that vector space.
}

\definition{Canonical basis of $\mathbb{R}^n$}{
The \textit{canonical basis} of the vector space of real $n$-tuples, $\mathbb{R}^n$, is the ordered set of $n$ $n$-tuples with $k^{th}$ element, $\mathbf{c}_k=(\alpha_1, \dots, \alpha_n)$ such that 
\begin{align*}
\alpha_j = 
\begin{cases} 
1 & \text{for } j= k, \\
0 & \text{for } j\neq k.
\end{cases}
\end{align*}
That is, as a set the canonical basis is
\begin{align*}
\mathcal{C}_n=\{ 
(1, 0, \dots, 0 ), \,
(0, 1, \dots, 0 ), \,
\dots, \,
\underbrace{(0, 0, \dots, 0, \overbrace{1}^{k^{th} \text{ place}}, 0, \dots, 0 )}_{k^{th} \text{ tuple}}, \,
\dots, \,
(0, 0, \dots, 1)
\}.
\end{align*}
}


\definition{Canonical basis of $\mathcal{P}_n$}{
The \textit{canonical basis} of the vector space of polynomials with degree up to $n$, $\mathcal{P}_n$, is the ordered set of $n$ polynomials with $k^{th}$ element, $\mathbf{c}_k= x^k$. That is, as a set the canonical basis is
\begin{align*}
\mathcal{C}_n=\{ 
1, \,
x, \,
x^2, \,
\dots, \,
x^n
\}.
\end{align*}
}

\definition{Coordinates of a vector}{
Let $\mathbf{v}$ be a vector in a vector space $V$. The coordinates of $\mathbf{v}$ \textit{with respect to a given basis} $\mathcal{B}$, denoted $\left[\mathbf{v}\right]_\mathcal{B}$, is a column of the unique set of coefficients in the linear combination of $\mathbf{v}$ in terms of the basis vectors.
}
